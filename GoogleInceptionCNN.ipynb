{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogleInceptionCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9T7hu1jgzL-",
        "colab_type": "code",
        "outputId": "25193e5e-e0ad-4925-d771-ebdaa8c820ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Nov 15 16:41:16 2019\n",
        "\n",
        "@author: avinashshanker\n",
        "\"\"\"\n",
        "# try:\n",
        "#   # %tensorflow_version only exists in Colab.\n",
        "#   %tensorflow_version 2.x\n",
        "# except Exception:\n",
        "#   pass\n",
        "# # Load the TensorBoard notebook extension\n",
        "# %tensorflow_version 2.x\n",
        "%load_ext tensorboard\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Fri Nov 15 16:41:16 2019\n",
        "\n",
        "@author: avinashshanker\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import warnings\n",
        "from keras.datasets import cifar10\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "from keras.layers import Flatten, Activation, Conv2D, MaxPool2D, AvgPool2D, Dense, Dropout, BatchNormalization, Input, MaxPooling2D, Flatten, Activation, Conv2D, AvgPool2D, Dense, Dropout, concatenate, AveragePooling2D\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.models import Sequential\n",
        "import keras.backend as K\n",
        "from keras.regularizers import l1,l2\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "from keras.models import model_from_json, Model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(390)\n",
        "\n",
        "#import CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "#Show classes of input dataset\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "nClasses = 10\n",
        "fig = plt.figure(figsize=(15,5))\n",
        "for i in range(nClasses):\n",
        "    ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])\n",
        "    idx = np.where(y_train[:]==i)[0]\n",
        "    features_idx = x_train[idx,::]\n",
        "    img_num = np.random.randint(features_idx.shape[0])\n",
        "    im = features_idx[img_num,::]\n",
        "    ax.set_title(class_names[i])\n",
        "    plt.imshow(im)\n",
        "plt.show()\n",
        "\n",
        "#Converting to float32 data type\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#This is temporary to slice while running on CPU, On GPU please comment\n",
        "# x_train = x_train[0:5000]\n",
        "# x_test = x_test[0:5000]\n",
        "# y_test = y_test[0:5000]\n",
        "# y_train = y_train[0:5000]\n",
        "\n",
        "#normalization of Training dataset\n",
        "# mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "# std = np.std(x_train,axis=(0,1,2,3))\n",
        "# x_train = (x_train-mean)/(std+1e-7)\n",
        "# x_test = (x_test-mean)/(std+1e-7)\n",
        "\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_test_cat = to_categorical(y_test)\n",
        "\n",
        "def CreateDense(input_layer, neurons_nr, dense_nr, dropout=False, normalization=False, regularization='l2', dropout_ratio=0.5):\n",
        "    dense = Dense(neurons_nr, kernel_regularizer=regularization, name='dense_%d_%d'%(dense_nr, neurons_nr))(input_layer)\n",
        "    \n",
        "    if dropout:\n",
        "        dense = Dropout(dropout_ratio, name='dense_%d_%ddrop'%(dense_nr, neurons_nr))(dense)\n",
        "    if normalization:\n",
        "        dense = BatchNormalization(name='dense_%d_%dnorm'%(dense_nr, neurons_nr))(dense)\n",
        "    \n",
        "    return dense\n",
        "\n",
        "def InceptionLayer(input_layer, features_nr, module_nr, dropout=False, normalization=False, regularization='l2', dropout_ratio=0.2):  \n",
        "  \n",
        "    inception_1x1 = Conv2D(features_nr[0],1,1,border_mode='same',activation='relu',name='inception_%d_/1x1'%(module_nr),W_regularizer=l2(0.0002))(input_layer)\n",
        "    inception_3x3_reduce = Conv2D(features_nr[1],1,1,border_mode='same',activation='relu',name='inception_%d_/3x3_reduce'%(module_nr),W_regularizer=l2(0.0002))(input_layer)    \n",
        "    inception_3x3 = Conv2D(features_nr[2],3,3,border_mode='same',activation='relu',name='inception_%d_/3x3'%(module_nr),W_regularizer=l2(0.0002))(inception_3x3_reduce)    \n",
        "    inception_5x5_reduce = Conv2D(features_nr[3],1,1,border_mode='same',activation='relu',name='inception_%d_/5x5_reduce'%(module_nr),W_regularizer=l2(0.0002))(input_layer)   \n",
        "    inception_5x5 = Conv2D(features_nr[4],5,5,border_mode='same',activation='relu',name='inception_%d_/5x5'%(module_nr),W_regularizer=l2(0.0002))(inception_5x5_reduce)    \n",
        "    inception_pool = MaxPooling2D(pool_size=(3,3),strides=(1,1),border_mode='same',name='inception_%d_/pool'%(module_nr))(input_layer)    \n",
        "    inception_pool_proj = Conv2D(features_nr[5],1,1,border_mode='same',activation='relu',name='inception_%d_/pool_proj'%(module_nr),W_regularizer=l2(0.0002))(inception_pool)   \n",
        "    inception_output = concatenate([inception_1x1,inception_3x3,inception_5x5,inception_pool_proj],axis=3,name='inception_%d_/output'%(module_nr))\n",
        "\n",
        "    if dropout:\n",
        "        inception_output = Dropout(dropout_ratio, name='inception_%d_/output_drop'%(module_nr))(inception_output)\n",
        "    if normalization:\n",
        "        inception_output = BatchNormalization(name='inception_%d_/output_norm'%(module_nr))(inception_output)\n",
        "\n",
        "    pooled = MaxPooling2D((2,2), padding='same', name='inception_%d_2x2subsample'%(module_nr))(inception_output)\n",
        "    \n",
        "    return pooled\n",
        "\n",
        "\n",
        "\n",
        "use_norm = True\n",
        "lrate = 0.001\n",
        "import keras\n",
        "input_img = keras.layers.Input(shape = (32, 32, 3), name='input')\n",
        "inception_1 = InceptionLayer(input_img, [64,96,128,16,32,32], 1, False, use_norm)\n",
        "inception_2 = InceptionLayer(inception_1, [128,128,192,32,96,64], 2, False, use_norm)\n",
        "inception_3 = InceptionLayer(inception_2, [192,96,208,16,48,64], 3, False, use_norm)\n",
        "inception_4 = InceptionLayer(inception_3, [160, 112, 224, 24, 64, 64], 4, False, use_norm)\n",
        "\n",
        "\n",
        "flat_pool = AveragePooling2D(pool_size=(2, 2), padding='valid')(inception_4)\n",
        "\n",
        "flat = Flatten()(flat_pool)\n",
        "\n",
        "dense_6 = CreateDense(flat, 64, 2, True, use_norm)\n",
        "\n",
        "out = Dense(10, activation='softmax')(dense_6)\n",
        "\n",
        "model = Model(inputs = input_img, outputs = out)\n",
        "\n",
        "# model.compile(loss='categorical_crossentropy',\n",
        "#               optimizer=Adam(lrate),\n",
        "#               #metrics=['top_k_categorical_accuracy']\n",
        "#               metrics=['top_k_categorical_accuracy'])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(0.001),\n",
        "              metrics=['accuracy','top_k_categorical_accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "name = \"inception-{}\".format(datetime.datetime.now())\n",
        "\n",
        "a = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
        "#b = ModelCheckpoint(monitor='val_loss', filepath='./models/'+str(i)+'.hdf5', verbose=1, save_best_only=True)\n",
        "c = TensorBoard(log_dir = os.path.join(\n",
        "    \"topk\",\n",
        "    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
        "                write_grads=True,\n",
        "                write_graph=True,\n",
        "                write_images=True,\n",
        "                batch_size=128)\n",
        "\n",
        "d = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
        "\n",
        "callbacks=[a,c,d]\n",
        "\n",
        "\n",
        "history = model.fit(x_train, y_train_cat, batch_size=128, epochs=20, validation_split=0.2,verbose=1,callbacks=callbacks)\n",
        "!cp -R logs my_drive/Colab\\ Notebooks/cifar10 \n",
        "\n",
        "# Loss Curves\n",
        "plt.style.use('ggplot')\n",
        "plt.figure(figsize=[8,6])\n",
        "plt.plot(history.history['loss'],'green',linewidth=3.0)\n",
        "plt.plot(history.history['val_loss'],'orange',ls = '--', linewidth=3.0)\n",
        "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Loss',fontsize=16)\n",
        "plt.title('Loss Curve',fontsize=16)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxZdgIKX5onH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Accuracy Curve\n",
        "plt.style.use('ggplot')\n",
        "plt.figure(figsize=[8,6])\n",
        "plt.plot(history.history['acc'],'red',linewidth=3.0)\n",
        "plt.plot(history.history['val_acc'],'blue',ls = '--',linewidth=3.0)\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18, loc = 'lower right')\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Accuracy',fontsize=16)\n",
        "plt.title('Accuracy Curves',fontsize=16)\n",
        "\n",
        "\n",
        "score = model.evaluate(x_test, y_test_cat, verbose=1)\n",
        "print(\"Test Loss: %.2f%%\" % (score[0]*100))\n",
        "print(\"Test Accuracy: %.2f%%\" % (score[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WbAxvJO5tfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Confusion matrix result\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "Y_pred = model.predict(x_test, verbose=2)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "for ix in range(10):\n",
        "    print(ix, confusion_matrix(np.argmax(y_test_cat,axis=1),y_pred)[ix].sum())\n",
        "confusion_matrix = confusion_matrix(np.argmax(y_test_cat,axis=1),y_pred)\n",
        "print(confusion_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDpS5VjlPZMe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "figure = plt.figure(figsize=(10, 10))\n",
        "import seaborn as sns\n",
        "sns.heatmap(confusion_matrix,  annot=True, fmt=\"d\", cmap='YlGnBu')\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skJWSznpc2XU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir topk\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}